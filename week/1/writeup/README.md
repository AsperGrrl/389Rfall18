Writeup 1 - Ethics
======

Name: Annette Keller
Section: 0201

I pledge on my honor that I have not given or received any unauthorized assistance on this assignment or examniation.

Digital acknowledgement: Annette Keller

## Assignment Writeup

### Part 1 (25 pts)

This was done via the [ELMS assignment](https://myelms.umd.edu/courses/1251976/assignments/4726433).

url submitted: https://github.com/AsperGrrl/389Rfall18

### Part 2 (75 pts)

Who should you tell, if anybody?
Your boss, a consumer watchdog, the public? In what order?

Firstly, a component of engineering ethics is proper documentation of lab results. The example given involves the discovery of an (automotive engineering) system with traffic safety concerns, and so engineering professional standards should also apply to this work, and it's not purely a hacking/vulnerability situation. It seems to me that engineering ethics and professional standards are a good model for hacking professionals and that a legally admissible & complete lab notebook, that's something like an engineering notebook, should be kept up during the work. In engineering firms, the engineer's lab notebook is proprietary material that belongs to the employer, as well as standing legal documentation of discoveries & processes in case of patent disputes, and so the vulnerability tester should have as their first report to the company complete documentation of the vulnerability & the process used to discover it, the lab notebook. A good lab notebook & lab records can serve as a deterrent against a company hiding engineering problems. So that's one line of defense against corruption of engineering process.

Consider, also, the opposite scenario: you haven't kept a formal lab notebook & lab results. The credibility can & will be attacked by anyone seeking to shake off responsibility for ignoring you.

Secondly, the vulnerability should be reported to (1) the technical supervisor of the vulnerability testing program and also, I would suggest, (2) the managerial supervisor of the safety issues for the department or division within which the vulnerability testing project was commissioned. Also, the technical & managerial supervisors of groups responsible for the affected part(s) and systems(s) (e.g. braking system) affected by the vulnerability must receive direct or copied notices of the vulnerability. This is what I would consider to be the formal reporting of scary results.

Failing to distribute the report to those responsible for the system/part, on the other hand, makes it more likely that the only people making decisions to ignore your report are not stakeholders in the long game of success of the system affected by the vulnerability.

Thirdly, while going to consumer groups & the public may be satisfying, generally, technical personnel working on proprietary systems will have signed a Non-Disclosure Agreement with their employer that prevents them from spewing information out to newspapers, consumer watchdogs, competitors & the public. I wouldn't consider it ethical or legal to break that contract frivolously.

You & your attempt to blow the whistle may lose credibility as a whistleblower if the company can retaliate against you & prosecute you in civil or criminal court.


How far should you go, if at all?

As I feel above, the first round of reporting (even if you have to stretch your network) should be to those managers & supervisors responsible for (1) the testing project, (2) safety within the division or department that had commissioned the testing project, and (3) the division or department that is engineering or designing the affected parts or systems (e.g. braking systems). Once you go above that level, you're invading executive space with your problem, which can turn out badly without helping your cause.


What if your company is unwilling to postpone the ECU's release?
If you do nothing, are you responsible for any harm that results? Why or why not?

This is both a legal & ethical question. Most corporations, and increasingly, most of American government, has become 'compliance oriented' with respect to obeying the law. I.e. rather than following internal rules or external laws and regulations, they follow a standard set by "I only avoid doing what would be easy to prove in court". 

In a compliance-oriented professional culture, if a lawyer can assert a feasible defense against prosecution for some sketchy or negligent action, then that action is effectively "legal". I.e. it's what cannot be defended in a court of law with your team of lawyers that forms the bright line corporations follow, whether or not that line actually rises to the level of the corporation's internal rules or with external laws & reguations. As pervasive as this culture of accountability based on what is defensible rather than what is actually defined by rules of safety & other regulations or standards, it makes a difference what the values are in the community affected by the vulnerability you discovered, since the bright lines companies follow are effectively set by what can be successfully prosecuted in civil litigation in the state courts or local federal circuit court. Accordingly, if you're in California, where the local politicians, businesses & people in a potential jury pool invest in safety & environmental design standards, even national standards for safety in that community are effectively higher than in other states. 

In other words, it makes a difference where the legal enforcement of the safety standard, your non-disclosure agreement and/or the civil liability for unsafe engineering will be adjudicated. If I'm in California, where people care a great deal about high safety & environmental standards & their regulations reflect the community's values, I'd be more likely to take risks to disclose the vulnerability than if I were in, say, Arkansas where people are less likely to care about safety if it makes their cars a little more expensive. This is what I would refer to as my "serve the community" principle. If the community doesn't value what you've discovered, you're not really helping them by forcing disclosure of vulnerabilities & safety problems -- you're helping yourself & things may not turn out as you hope.

So my answer on what I would do if the company was unwilling to correct the vulnerabilty is also compliance oriented, i.e. what the local state's laws & courts set as bright lines. But not because I believe that legality equals ethics, but because local laws & court cases reflect the community's values & priorites. If I were in Germany, California, or France, I'd take certain more risks to disclose the information than if I'm in Donald Trump's America, where large majorities have clearly voted against safety & environmental regulations in the expectation that his anti-regulatory extremism will create more jobs & put a few more dollars in their wallet. In that way, a state's laws & regulations provide a guide to your community's values that effectively translate to inform how you handle your professional conflicts. For professional conflicts over safety vs disclosure, therefore, analysis of my own professional risk would be informed by the local courts & the community that supports the laws & enforcement therefor.

In my experience, the idealism of a greater good to which a whistleblower can invest herself or himself in America today is a moving target. As part of the era in which we live, public health & safety ethics have devolved into the naked self-interest of polarized interest groups and if you're going to take risks to be a whistleblower, you had better have a stakeholder, or community of stakeholders, who give(s) a shit about what you discovered and whose values are invested in that issue, be it safety, quality or health.

In a nutshell, I'd follow my professional & business contractual obligations (especially because of a likely nondisclosure agreement & the fact that the information would belong to the company & would likely be proprietary. If my employer was being unsafe & negligent, I may skate toward taking risks for public disclosure. If the community cares, that would make a difference to me. 

It can be argued that ideals are absolute & universal, like automotive safety that meets high standards. But in a corporate environment, you're not an individual but part of a structured community where others have authority too. It's important to recognize that ethics should help determine who you choose to work for & what communities you choose to live in, and getting involved with an automotive company with low standards is an a priori decion about your values. If you make decisions about who to work for & where to live based things other than the company's values & standards and your community's values, you may find yourself stuck with a post hoc error correction problem where you find yourself in the wrong company or community. This is also why individuals should be engaged in public policy discourse, voting and other community shaping civil engagement. Whistleblowing in isolated circumstances is a short-term game, whereas civic engagement & ethical choices are how people play the long game to improve public safety, health & welfare. Those who are concerned about safety, health, environment & ethics should vote & speak up in ways that are one's civic right and responsibility to the community. How we vote impacts the ethics of our executive class of decision-makers more than our individual pursuit of ideals as whistleblowers. Civic engagement is one of the activities in which ideals can be treated as absolutes & universal, as opposed to the kind of activities where you break contracts you've signed or give away a company's proprietary data to the public and its competitors. 

People also choose what products to buy, and they know when they chose a car made by a company focused on safety or when they're supporting a company that isn't. People who buy based on wow factors or fail to research a car company before buying, inflict the baggage of uninformed consumerism on those who have to fight for standards. That's a lot of baggage for one individual professional to carry if they find themselves in a car company that would make unsafe decisions (and any consumer who checks out their quality & reliablity can see that history). We all see people who choose to pay less for more, ignoring that quality or ethical corners were cut. Wal-mart wouldn't be successful if people who wanted to save a few cents on the dollar weren't okay with the notion that it takes virtual slaves in other countries to produce Wal-mart's cheaper goods that undercut other discounters who import from the same countries. In America, unethical consumerism is a norm, and it's difficult to argue that the problems that arise from it must be carried by individuals on a security quest. We live in a mass consumption country where consumer choices become political & social drivers of what we mean when we talk about corruption, failure to meet standards & compromises of the ethics with which products are produced. Consumer driven support for poor business standards in companies that deliver more features for the same price than a better-quality product is consistent enough in our Wal-mart consumer culture that it either incentivizes or limits what you see happening around you in a corporate environment. I.e. people who support shitty corporate products bear personal responsibility for their choices & keeping that company in business, too, just as do those who decide to work for them.

Of course we all want to do the right thing, and grey hat hacking sometimes provides innovative solutions for those who want to accomplish something of sketchy legality for the greater good. If you're considering violating non-disclosure agreements & professional contracts for public disclosure, then you're already contemplating violating civil laws to accomplish a greater good.

